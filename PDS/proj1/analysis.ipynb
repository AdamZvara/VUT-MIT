{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from src.DataLoader import DataLoader\n",
    "from src.FeatureExtraction import FeatureExtraction\n",
    "from src.LogCluster import LogCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use feature extraction on only valid instances and see, if anomalies are separable from normal logs (with different weighting types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(False)\n",
    "x_raw, y_raw = dl.load_csv('data/HDFS100k/log_structured.csv', 'data/HDFS100k/log_labels.csv')\n",
    "y_raw = np.array(y_raw)\n",
    "\n",
    "fe = FeatureExtraction(\"EventId\", False)\n",
    "x, y = fe.session_windowing(x_raw[y_raw == 0], r'(blk_-?\\d+)', \"Content\", y_raw)\n",
    "\n",
    "x_validate, y_validate = fe.transform(x_raw, y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing basic info about files ... we can see, that normal logs are missing some events (E7, E8 ... E27), which usually contain some keywords like \"exception\" and \"interrupt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print characteristics of the dataset\n",
    "print(\"Number of sessions: \", len(x_validate))\n",
    "print(\"Normal sessions: \", len([i for i in y_validate if i == 0]))\n",
    "print(\"Anomalous sessions: \", len([i for i in y_validate if i == 1]))\n",
    "\n",
    "print(\"Number of unique event types: \", len(fe.events))\n",
    "\n",
    "print()\n",
    "\n",
    "normal = x_validate[y_validate == 0].sum(axis=0)\n",
    "normal_events = normal[normal > 0].index.values\n",
    "print(f\"Unique events in normal sessions: {len(normal_events)} -> {normal_events}\")\n",
    "\n",
    "anomaly = x_validate[y_validate == 1].sum(axis=0)\n",
    "anomaly_events = anomaly[anomaly > 0].index.values\n",
    "print(f\"Unique events in anomalous sessions: {len(anomaly_events)} -> {anomaly_events}\")\n",
    "\n",
    "anomaly_events = set(anomaly_events) - set(normal_events)\n",
    "print(f\"Unique events in anomalous sessions not in normal sessions: {len(anomaly_events)} -> {anomaly_events}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Print normal event templates\n",
    "print(\"E2: Verification succeeded for <*>\")\n",
    "print(\"E3: Served block <*> to /<*>\")\n",
    "print(\"E5: Receiving block <*> src: /<*> dest: /<*>\")\n",
    "print(\"E6: Received block <*> src: /<*> dest: /<*> of size <*>\")\n",
    "print(\"E9: Received block <*> of size <*> from /<*>\")\n",
    "print(\"E11: PacketResponder <*> for block <*> terminating\")\n",
    "print(\"E16: <*>:Transmitted block <*> to /<*>\")\n",
    "print(\"E18: <*> Starting thread to transfer block <*> to <*>\")\n",
    "print(\"E21: Deleting block <*> file <*>\")\n",
    "print(\"E22: BLOCK* NameSystem.allocateBlock:<*>\")\n",
    "print(\"E25: BLOCK* ask <*> to replicate <*> to datanode(s) <*>\")\n",
    "print(\"E26: BLOCK* NameSystem.addStoredBlock: blockMap updated: <*> is added to <*> size <*>\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"E7: writeBlock <*> received exception <*>\")\n",
    "print(\"E8: PacketResponder <*> for block <*> Interrupted.\")\n",
    "print(\"E10: PacketResponder <*> <*> Exception <*>\")\n",
    "print(\"E13: Receiving empty packet for block <*>\")\n",
    "print(\"E14: Exception in receiveBlock for block <*> <*>\")\n",
    "print(\"E15: Changing block file offset of block <*> from <*> to <*> meta file offset to <*>\")\n",
    "print(\"E27: BLOCK* NameSystem.addStoredBlock: Redundant addStoredBlock request received for <*> on <*> size <*>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average counts of events in normal sessions: \\n\", (normal / len(x_validate[y_validate == 0])).sort_values(ascending=False).to_frame().T.to_string())\n",
    "print()\n",
    "print(\"Average counts of events in anomalous sessions: \\n\", (anomaly / len(x_validate[y_validate == 1])).sort_values(ascending=False).to_frame().T.to_string())\n",
    "print()\n",
    "print(\"Differences in average counts of events in normal and anomalous sessions: \\n\", (normal / len(x_validate[y_validate == 0]) - anomaly / len(x_validate[y_validate == 1])).abs().sort_values(ascending=False).to_frame().T.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the different occurences of events in normal vs anomalous data ... for better clarity remove duplicate sequences. We can see, how much each event occurs in normal vs anomaly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_deduplicated = x_validate[y_validate == 0].drop_duplicates()\n",
    "anomaly_deduplicated = x_validate[y_validate == 1].drop_duplicates()\n",
    "\n",
    "def plot_features(norm, anom):\n",
    "    norm[\"Label\"] = \"Normal\"\n",
    "    anom[\"Label\"] = \"Anomaly\"\n",
    "    all = pd.concat([norm, anom])\n",
    "    print(all.to_string())\n",
    "\n",
    "    low = all[all[\"E3\"] < 200] # filter 2 events with very high counts to make the plot more readable\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    \n",
    "    def plot_group(group, label):\n",
    "        col = \"blue\" if label == \"Normal\" else \"orange\"\n",
    "        lw = 3 if label == \"Normal\" else 1\n",
    "        style = \"-\" if label == \"Normal\" else \"--\"\n",
    "        ax.plot(all.columns[:-1].values, group.T.values, color=col, linewidth=lw, linestyle=style, label=[label if i == 0 else \"_nolabel_\" for i in range(len(group))])\n",
    "        \n",
    "    plot_group(low[low[\"Label\"]  == \"Normal\"].drop(columns=[\"Label\"]), \"Normal\")\n",
    "    plot_group(low[low[\"Label\"]  == \"Anomaly\"].drop(columns=[\"Label\"]), \"Anomaly\")\n",
    "    \n",
    "    list(map(lambda x: x.set_color('green'), ax.get_xticklabels()))\n",
    "    anomaly_labels = filter(lambda x: x.get_text() in anomaly_events, ax.get_xticklabels())\n",
    "    for i in anomaly_labels:\n",
    "        i.set_color('red')\n",
    "        i.set_weight('bold')\n",
    "        \n",
    "    ax.legend()\n",
    "        \n",
    "    return fig\n",
    "        \n",
    "fig = plot_features(normal_deduplicated, anomaly_deduplicated)\n",
    "# fig.savefig(\"count_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does inverse term document frequency influence the difference in logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_w = fe.apply_weighting(x_validate, True)\n",
    "normal_w = x_w[y_validate == 0].drop_duplicates()\n",
    "anomaly_w = x_w[y_validate == 1].drop_duplicates()\n",
    "\n",
    "# print(anomaly_w)\n",
    "print(normal[normal > 10])\n",
    "\n",
    "fig = plot_features(normal_w, anomaly_w)\n",
    "# fig.savefig(\"idf_weighting.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how does contrast weighting influence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_wc = fe.apply_weighting(x_validate, True, True)\n",
    "normal_wc = x_wc[y_validate == 0].drop_duplicates()\n",
    "anomaly_wc = x_wc[y_validate == 1].drop_duplicates()\n",
    "\n",
    "fig = plot_features(normal_wc, anomaly_wc)\n",
    "# fig.savefig(\"idf_weighting_context.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply clustering with contrast weights and see, how similiar are anomalous events to normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = fe.apply_weighting(x, True, True)\n",
    "\n",
    "model = LogCluster(0.000001, 0, True, True)\n",
    "model.fit(x2)\n",
    "\n",
    "def distance(model, x):\n",
    "    centroids = model.centroids.copy()\n",
    "    centroids.insert(0, x)\n",
    "    # Calculate cosine distance to all centroids\n",
    "    dist = squareform(pdist(centroids, metric='cosine'))[0]\n",
    "    min_dist = np.min(dist[1:])\n",
    "    return min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._synchronize_events(anomaly_wc.drop(columns=[\"Label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal sessions:\")\n",
    "for i in normal_wc.iterrows():\n",
    "    i = i[1].to_list()[:-1]\n",
    "    print(distance(model, i))\n",
    "    \n",
    "print()\n",
    "    \n",
    "print(\"Anomalous sessions:\")\n",
    "result = []\n",
    "for i in anomaly_wc.iterrows():\n",
    "    cnt = 0\n",
    "    for j in x_wc.iterrows():\n",
    "        if y_validate[j[0]] == 0:\n",
    "            continue\n",
    "        if sum(list(map(isclose, i[1][:-1].to_list(), j[1].to_list()))) == len(i[1]) - 1:\n",
    "            cnt += 1\n",
    "    \n",
    "    i = i[1].to_list()[:-1]\n",
    "    result.append((cnt, distance(model, i)))\n",
    "    \n",
    "allsum = 0\n",
    "for i in result:\n",
    "    print(f\"Counts: {i[0]}, Distance: {i[1]}\")\n",
    "    allsum += i[0]\n",
    "\n",
    "print(\"Overall anomalous sessions (checking correctness): \", allsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even different weighted normal events can be used to detect anoamlies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x_wc.iloc[0].copy()\n",
    "print(\"Normal sequence:\")\n",
    "print(data.to_frame().T.to_string())\n",
    "print(\"Distance\", distance(model, data))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Modified sequence:\")\n",
    "data[\"E2\"] = 0.5\n",
    "print(data.to_frame().T.to_string())\n",
    "print(\"Distance\", distance(model, data))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Modified sequence 2:\")\n",
    "data[\"E2\"] = 1\n",
    "print(data.to_frame().T.to_string())\n",
    "print(\"Distance\", distance(model, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for some anomalous events, the distance is quite large (i am suspecting that those are the events that are nicely separated in graphs) .. on the other hand there are handful of events, which pretty much exactly match the input data .. thats probably the reason why is LogCluster ineffective (low recall). To test this, get blockIds from \"nicely separable\" anomalies and create synthetic data (data/synthetic/log_structured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "res = []\n",
    "for idx, row in x_raw.iterrows():\n",
    "    blkId = re.findall(r'(blk_-?\\d+)', row[\"Content\"])[0]\n",
    "    if blkId not in res:\n",
    "        res.append(blkId)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in anomaly_wc.iterrows():\n",
    "    i = i.to_list()[:-1]\n",
    "    if distance(model, i) > 0.1:\n",
    "        print(res[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
